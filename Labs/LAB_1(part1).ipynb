{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK24ld7r1fFh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Algebra Lab: Foundations for Deep Learning\n",
        "**Duration:** 60 Minutes  \n",
        "**Objective:** Mastering Matrix Operations as the Building Blocks of Neural Networks.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 1: Matrix Architecture & Symmetry (10 Minutes)\n",
        "*In Deep Learning, matrix properties dictate how data is transformed. Symmetry and identity play key roles in \"Residual Connections\" and \"Distance Metrics.\"*\n",
        "\n",
        "**Q1. Identity vs. Scalar:** Is every **Identity matrix** a **Scalar matrix**? Is every **Scalar matrix** an **Identity matrix**? Provide a $3 \\times 3$ example.  \n",
        "> **DL Note:** The Identity matrix ($I$) is used in \"Skip Connections\" (ResNets) to allow gradients to flow through layers without being altered.\n",
        "\n",
        "**Q2. Matrix Construction:** Construct a $3 \\times 3$ matrix $A = [a_{ij}]$ where $a_{ij} = i^2 - j^2$.  \n",
        "1. Write out the full matrix.\n",
        "2. Is it **Symmetric** ($A = A^T$) or **Skew-symmetric** ($A = -A^T$)?\n",
        "3. **Deep Learning Context:** If a weight matrix must be symmetric to ensure stable learning, how would the operation $W + W^T$ help?\n",
        "\n",
        "---\n",
        "\n",
        "## Part 2: High-Dimensional Operations (15 Minutes)\n",
        "*Matrix multiplication is the fundamental operation of a Neural Network layer: $y = f(Wx + b)$.*\n",
        "\n",
        "**Q3. The Dimensionality Constraint:** Let $X$ (Input Vector) be $1 \\times 4$. Let $W$ (Weight Matrix) be $4 \\times 3$.\n",
        "1. What is the order (dimensions) of the product $XW$?\n",
        "2. Can you calculate $WX$? Justify why this fails in the context of \"Input-to-Hidden\" mapping.\n",
        "\n",
        "\n",
        "\n",
        "**Q4. The Commutative Trap:** Given $A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$ and $B = \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix}$:\n",
        "1. Calculate $AB$ and $BA$.\n",
        "2. Is $AB = BA$?\n",
        "3. **Observation:** In a Deep Learning pipeline, if $A$ is a \"Rotation\" and $B$ is a \"Scaling\" operation, does the order in which you apply them matter?\n",
        "\n",
        "**Q5. Expansion of Squares:** Calculate $(A+B)^2$ and compare it to $A^2 + 2AB + B^2$. Why are they different in matrix algebra compared to standard scalar algebra?\n",
        "\n",
        "---\n",
        "\n",
        "## Part 3: Determinants & Information Loss (10 Minutes)\n",
        "*Determinants tell us if a layer \"collapses\" the data into a lower dimension, which can lead to information loss.*\n",
        "\n",
        "**Q6. Calculation:** For $A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 4 \\\\ 5 & 6 & 0 \\end{bmatrix}$, find $|A|$ using expansion along the first row.\n",
        "\n",
        "**Q7. The Scaling Property:** If $|A| = 5$ for a $3 \\times 3$ weight matrix, find $|2A|$.  \n",
        "> **Formula:** $|kA| = k^n |A|$ where $n$ is the order.  \n",
        "> **DL Insight:** If the determinant of your weight matrix is very small ($< 1$), what might happen to the magnitude of your data as it passes through many layers?\n",
        "\n",
        "---\n",
        "\n",
        "## Part 4: Transposition & Backpropagation (10 Minutes)\n",
        "*Transposition is the \"reverse gear\" of linear algebra, used during the training phase to calculate gradients.*\n",
        "\n",
        "**Q8. The \"Sock-and-Shoe\" Property:** Given $A = \\begin{bmatrix} 1 & 5 \\\\ 6 & 7 \\end{bmatrix}$ and $B = \\begin{bmatrix} 2 & 0 \\\\ 1 & 3 \\end{bmatrix}$, show that $(AB)^T = B^T A^T$.  \n",
        "*This property is essential when calculating the gradient of a loss function with respect to weights.*\n",
        "\n",
        "**Q9. Adjoint & Minors:** For $M = \\begin{bmatrix} 1 & 0 & 2 \\\\ 3 & -1 & 1 \\\\ 4 & 5 & 6 \\end{bmatrix}$:\n",
        "1. Find the **Minor** and **Cofactor** of $a_{23}$.\n",
        "2. Explain how the **Adjoint** ($Adj A$) relates to the inverse of a matrix.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 5: Inversion & System Stability (15 Minutes)\n",
        "*Inverting a matrix is equivalent to \"undoing\" a transformation. If a matrix cannot be inverted, the transformation is \"lossy.\"*\n",
        "\n",
        "**Q10. The Inverse:** Calculate $A^{-1}$ for $A = \\begin{bmatrix} 2 & 5 \\\\ 1 & 3 \\end{bmatrix}$ using the formula $A^{-1} = \\frac{1}{|A|} (Adj A)$. Verify that $A \\cdot A^{-1} = I$.\n",
        "\n",
        "**Q11. Solving Systems:** Use the Matrix Inversion Method ($X = A^{-1}B$) to solve for the input features $x$ and $y$:\n",
        "$$x + 2y = 4$$\n",
        "$$2x + 5y = 9$$\n",
        "\n",
        "**Q12. The Singular Matrix Crisis:** Consider $S = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix}$.\n",
        "1. Calculate $|S|$.\n",
        "2. Why is it impossible to find $S^{-1}$?\n",
        "3. **Geometric Interpretation:** If a layer uses matrix $S$, it collapses 2D space into a 1D line. Can we recover the original 2D data after this collapse?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**End of Lab Session**"
      ],
      "metadata": {
        "id": "TbFqHsQ91h79"
      }
    }
  ]
}